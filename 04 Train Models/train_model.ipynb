{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029abeac-4e2c-4d5f-8daf-795d5b78b245",
   "metadata": {},
   "source": [
    "# Notebook: Train Model\n",
    "\n",
    "This notebook is used to train a classification model given a dataset of tweets. Results of the training are saved in CSV and JSON.\n",
    "<br>**Contributors:** [Nils Hellwig](https://github.com/NilsHellwig/) | [Markus Bink](https://github.com/MarkusBink/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf5bc3-0afa-4583-84a5-b970a691d42c",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c4500d-fd29-4d68-ac75-1ff84c8f5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from get_germeval_2017_dataset import get_germeval_2017_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30e67d-c7a8-4c0d-b5c2-0cc72df424a8",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a5a837-0abe-49ab-9c24-3e0e8f8cd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_ID = 0\n",
    "TEST_DATASET_PATH = f'../Datasets/k_fold_splits/TRAIN_TEST_{SPLIT_ID}/test.csv'\n",
    "N_TRAIN_EPOCHS = 4\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32\n",
    "USE_CUDA = False\n",
    "SEED_VALUE = 0\n",
    "MODEL_TYPE = \"bert\"\n",
    "MODEL_NAME = \"deepset/gbert-base\"\n",
    "MODEL_DIRECTORY_PATH = \"output\"\n",
    "PATH_RESULT_DATA = f'../Models/Results/GermEval_and_Annotaded_it_{SPLIT_ID}'\n",
    "SAVE_MODEL = False\n",
    "N_LABELS = 2\n",
    "EVALUATE_MODEL = True\n",
    "LABEL_DEFINITION = {'negative': 1, 'positive': 0, 'neutral': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7383c58-6d48-4708-8379-394d66011e58",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66018039-82cd-4539-b734-d09daf2c82e6",
   "metadata": {},
   "source": [
    "### 1. Get Reproducable Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3e5faa-0c46-4860-9093-0b0dcb038ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c9a68-c98f-42f8-b74a-ff2b69c3b117",
   "metadata": {},
   "source": [
    "### 2. Load Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc7d49-fc22-43f2-abb9-ce7c7a71a073",
   "metadata": {},
   "source": [
    "#### Load Training Data\n",
    "**Important:** Comment out unnecessary data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40d4b85-2e42-4084-9f7c-52d415883dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_annotated_split = pd.read_csv(f'../Datasets/k_fold_splits/TRAIN_TEST_{SPLIT_ID}/train.csv', encoding=\"utf-8\")[[\"tweet\",\"sentiment_label\"]].rename(columns={\"tweet\":\"text\"})\n",
    "train_df_germeval = get_germeval_2017_dataset()\n",
    "train_df_annotated_total = pd.read_csv(\"../Datasets/annotations.csv\", encoding=\"utf-8\")[[\"tweet\",\"sentiment_label\"]].rename(columns={\"tweet\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59f8e92d-a309-4b77-8e20-4e9917aaaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df_annotated_split, train_df_germeval], axis=0).sample(frac=1, random_state=SEED_VALUE).reset_index(drop=True)\n",
    "train_df['sentiment_label'] = train_df['sentiment_label'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc09df-1410-4a5a-8d61-435f66773fa5",
   "metadata": {},
   "source": [
    "Check Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e8ff5-eee6-4fdb-aeb5-270f04d4edb9",
   "metadata": {},
   "source": [
    "#### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a82fd1a-ee55-44bf-a8fe-7ab5086292ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODEL:\n",
    "    test_df = pd.read_csv(TEST_DATASET_PATH, encoding=\"utf-8\")[[\"tweet\",\"sentiment_label\"]].rename(columns={\"tweet\":\"text\"})\n",
    "    test_df['sentiment_label'] = test_df['sentiment_label'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c825411-125b-4f46-b944-e4c21b319a8b",
   "metadata": {},
   "source": [
    "#### Replace label strings with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19afe51-7b1e-4052-8640-2c5818221937",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentiment_label'] = train_df['sentiment_label'].replace(LABEL_DEFINITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea5c123-9d0b-4db0-92a9-a6e79dfde884",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODEL:\n",
    "    test_df['sentiment_label'] = test_df['sentiment_label'].replace(LABEL_DEFINITION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c2e7e-d6e4-42cf-9ddd-c8a860d0c7e0",
   "metadata": {},
   "source": [
    "### 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f771fd1-d08d-4f2c-974d-dd162bf098ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    \"fp16\":False,\n",
    "    \"num_train_epochs\":N_TRAIN_EPOCHS,\n",
    "    \"overwrite_output_dir\":True,\n",
    "    \"train_batch_size\":TRAIN_BATCH_SIZE,\n",
    "    \"eval_batch_size\":TEST_BATCH_SIZE,\n",
    "    \"manual_seed\": SEED_VALUE,\n",
    "    \"reprocess_input_data\":True,\n",
    "    \"no_save\":True,\n",
    "    \"no_cache\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b84f368-ca2c-45f2-b3e1-624842e30755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel(model_type=MODEL_TYPE, model_name=MODEL_NAME, num_labels=N_LABELS, args=training_args, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d0c66-f492-4fad-bebe-a8b3f31679e9",
   "metadata": {},
   "source": [
    "### 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c58ec18-1a76-40b7-a952-82716c4cd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba82ab-2980-49eb-a236-1e27150d3add",
   "metadata": {},
   "source": [
    "### 5. Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be68ca34-d469-4863-9f0c-6ce2bffc8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = accuracy_score\n",
    "\n",
    "def f1_metrics(labels, predictions):\n",
    "    metrics = {\n",
    "      \"f1_macro\": f1_score(labels, predictions, average='macro'),\n",
    "      \"f1_micro\": f1_score(labels, predictions, average='micro'),\n",
    "      \"f1_weighted\": f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def precision_metrics(labels, predictions):\n",
    "    metrics = {\n",
    "      \"precision_macro\": precision_score(labels, predictions, average='macro'),\n",
    "      \"precision_micro\": precision_score(labels, predictions, average='micro'),\n",
    "      \"precision_weighted\": precision_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def recall_metrics(labels, predictions):\n",
    "    metrics = {\n",
    "      \"recall_macro\": recall_score(labels, predictions, average='macro'),\n",
    "      \"recall_micro\": recall_score(labels, predictions, average='micro'),\n",
    "      \"recall_weighted\": recall_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "280a5715-fc3c-40cb-b802-4ba0db25e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_each_class(labels, predictions):\n",
    "    precision_recall = {}\n",
    "    for c in set(labels):\n",
    "        label_idx = [i for i, x in enumerate(labels) if x == c]\n",
    "        pred_idx = [i for i, x in enumerate(predictions) if x == c]\n",
    "        precision = len(set(label_idx).intersection(set(pred_idx))) / len(pred_idx) if len(pred_idx) > 0 else 0\n",
    "        recall = len(set(label_idx).intersection(set(pred_idx))) / len(label_idx) if len(label_idx) > 0 else 0\n",
    "        precision_recall[c] = {\"precision\": precision, \"recall\": recall}\n",
    "    return {\"precision_recall_each_class\": precision_recall}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884505d2-9d3a-407b-bd18-5fcaa3e6c1d5",
   "metadata": {},
   "source": [
    "### 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b63e51-ab2a-46fb-9ddb-522b7b5d31ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow_m1/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e955e0f0d7d472cb92d9599c8715338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ae01328ef24d4daa24a260e69d8f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if EVALUATE_MODEL:\n",
    "    result, model_outputs, wrong_predictions = model.eval_model(test_df, acc=accuracy_metric, f1=f1_metrics, precision=precision_metrics, recall=recall_metrics, precision_recall_each_class=precision_recall_each_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3422176-0aa6-4cd8-bf64-426d387a728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_MODEL:\n",
    "    with open(PATH_RESULT_DATA+\".json\", 'w') as f:\n",
    "        json.dump(result, f, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c93b7-1dbc-4348-8c35-b40f47bf71be",
   "metadata": {},
   "source": [
    "### 7. Save Evaluated Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ea3485a-adfd-4784-b7e6-e5889af1f364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc94ea352d94e25ad6f164b426cf97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2790f511f0148f8aa1bcf6d01e0c78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = test_df\n",
    "texts = []\n",
    "for index, row in test_data.iterrows():\n",
    "    texts.append(row[\"text\"])\n",
    "predictions, raw_outputs = model.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dcb4442-5dec-4b81-9487-1cb52b794977",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_out = test_df.assign(pred=pd.Series(predictions))\n",
    "test_df_out.to_csv(PATH_RESULT_DATA+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e5b55-1229-4406-acae-c985af7d1814",
   "metadata": {},
   "source": [
    "### 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05f0d2c9-6750-454d-a3e1-cb8cbdc827a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_MODEL:\n",
    "    model.save_model(MODEL_DIRECTORY_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
